% !TEX encoding = UTF-8 Unicode
\documentclass{article}

\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{hyperref}
\usepackage{color}
\usepackage{unicode}
\hypersetup{colorlinks}

\hyphenation{Berle-kamp}

%%%%%%%%%%%%%%%

\newcommand{\ff}[1]{\mathbb{F}_{#1}}
\newcommand{\fq}{\ff{q}}
\newcommand{\fqn}{\ff{q^n}}

\newcommand{\dd}{d}
\newcommand{\qq}{q}
\newcommand{\QQ}{Q}
\newcommand{\nn}{n}
\newcommand{\qn}{{\qq^\nn}}
\newcommand{\extfactfdegree}{k}
\newcommand{\extfactfsize}{\qq^{\nn \cdot \extfactfdegree}}

% if we define everything in terms of base field, extension field and
% extension field used in factorization
%
\newcommand{\basef}{\ff{\qq}}
\newcommand{\extf}{\ff{\qn}}
\newcommand{\extfactf}{\ff{\extfactfsize}}

\newcommand{\AG}{\mathrm{AG}(\qq,\nn)}

\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Ima}{Im} 
\DeclareMathOperator{\Decomp}{Decomp} 
\DeclareMathOperator{\Var}{Var} 
\DeclareMathOperator{\Exp}{E} 
\DeclareMathOperator{\loglog}{loglog}


% to specify the number of elements of the finite fields on which the
% trace is defined
\newcommand{\tr}[2]{\Tr_{\ff{#1}:\ff{#2}}}

% to specify the number of elements of the finite fields on which the
% trace is defined: light form
\newcommand{\trl}[2]{\Tr_{#1:#2}}

% to specify the notation of the finite fields on which the trace is
% defined
\newcommand{\trabs}[2]{\Tr_{#1:#2}}
\newcommand{\trextbase}{\trabs{\extf}{\basef}}
\newcommand{\trextfactext}{\trabs{\extfactf}{\extf}}
\newcommand{\trextfactbase}{\trabs{\extfactf}{\basef}}

\newcommand{\bigO}{O}
\newcommand{\bigOt}{\tilde{O}}
\newcommand{\smallO}{o}
\newcommand{\Mul}{\mathsf{M}}

\newcommand{\Span}{\mathbf{span}}
\newcommand{\card}[1]{\# #1}
\DeclareMathOperator{\Res}{Res}

\newcommand{\cost}[1]{\color{blue}Cost:  #1\color{black}}

%%%%%%%%%%%% Algorithms

\usepackage{float,algorithm}
\usepackage[noend]{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\newcounter{algo}

\newenvironment{algorithm_noendline}[4]{\begin{center}\begin{minipage}{0.48\textwidth}
      \refstepcounter{algo}
      \label{#4}
      \sf
      \rule{\textwidth}{0.2pt}\\
      \makebox[\textwidth][c]{Algorithm~\arabic{algo}:~\textbf{#1}}\\
      \rule[0.5\baselineskip]{\textwidth}{0.2pt}\\

      \vspace{-12pt}

      \parbox{\textwidth}{\textbf{Input} #2}
      \parbox{\textwidth}{\textbf{Output} #3}

\vspace{-7pt}

      \begin{enumerate*}}{\end{enumerate*}
      \vspace{-11pt}
\end{minipage}\end{center}
}

\newenvironment{algorithm_endline}[4]{\begin{center}\begin{minipage}{0.48\textwidth}
      \refstepcounter{algo}
      \label{#4}
      \sf
      \rule{\textwidth}{0.2pt}\\
      \makebox[\textwidth][c]{Algorithm~\arabic{algo}:~\textbf{#1}}\\
      \rule[0.5\baselineskip]{\textwidth}{0.2pt}\\

      \vspace{-12pt}

      \parbox{\textwidth}{\textbf{Input} #2}
      \parbox{\textwidth}{\textbf{Output} #3}

\vspace{-7pt}

      \begin{enumerate*}}{\end{enumerate*}
      \vspace{-11pt}
      \rule{\textwidth}{0.2pt}
\end{minipage}\end{center}
%\vspace{-0.5cm}
}

\floatstyle{plain}
\newfloat{algofloat}{thp}{bla}
\floatname{algofloat}{}

%%%%%%%%%%

\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\com }{\noindent \textcolor{blue}{Commentaire Micha\"el}:}
\newcommand{\comd}{\noindent \textcolor{blue}{D\'ebut Micha\"el}:}
\newcommand{\comf}{\noindent \textcolor{blue}{:Fin Micha\"el}}




\newtheorem{Def}{Definition}
\newtheorem{Theo}{Theorem}
\newtheorem{Prop}{Proposition}
\newtheorem{Lem}{Lemma}
\newtheorem{Coro}{Corollary}


\newcommand{\Notes}[1]{\textcolor{red}{Note: #1}}

\author{Luca De Feo, Christophe Petit, Micha\"el Quisquater}

\title{On root finding algorithms in finite fields}

\begin{document}

\maketitle
\begin{abstract}
  We find roots
\end{abstract}


\Notes{
Decisions to be made:
\begin{itemize}
\item notations $n$, $d$, $q$, $Q=q^n$ then $Q_i$ for subfields
\item complexity model: bit vs field operations -> we will keep small field operations complexity
\item Only analyze for fast polynomial arithmetic, use M(n) notation.
\item worst case vs average case -> both of them for deterministic algorithms
\item notation O tilde -> precise analysis, then simplified with O tilde
\item q is not O(1)
\item Traces: just write Tr when context is clear?
\end{itemize}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:introduction}

Let $\ff{q}$ be a finite field with $q$ elements, and let $f$ be a
polynomial of degree $d$ over $\ff{q}$.
%
The \emph{root-finding problem} is the problem of computing one,
several or all elements $x∈\ff{q}$ such that $f(x)=0$.
%
This problem has many applications, in particular in cryptography and
in coding theory~\cite{McEliece78} \Notes{either put more references,
  or less!}. It also has a rich history, with many strategies proposed
over the years. In this paper we review several algorithms for root
finding, and present a few new ones.  We highlight the relationships
between these algorithms by classifying them according to general
algorithmic principles, and using this classification to discover new
ones. We also present, when relevant, variants of the algorithms with
interesting properties. Finally, we implement most of the algorithms,
and compare their performances.

\subsection{A brief history of root finding}
\label{sec:brief-history-root}

As mentioned by von zur Gathen~\cite{Gathen06}, the first method for
finding the roots of $f∈\ff{q}[X]$, with $q$ an odd prime,
is due to Legendre (1752-1833) who starts by considering the
factorization of the field equation
$$X^q-X=X · (X^{(q-1)/2}-1) · (X^{(q-1)/2}+1).$$
He then observes that computing the GCD of $f$ with each of the two
latter factors splits the (nonzero) roots of $f$ into two sets, the
quadratic residues and the non-residues. Then, replacing $X$ by $X+r$
for a random $r$, yields other ways of splitting the roots into two
sets, which may recursively be applied on the factors of $f$. This
probabilistic method is the basis of many modern ones.

For a polynomial $f∈\ff{q}[X]$, with $q$ a prime power, the
factoring method of Berlekamp~\cite{berl67} is based on the so-called
\emph{Petr-Berlekamp algebra} of $f$ which consists of the set of
polynomials $h$ satisfying
$$h(X)^q=h(X) \mod{f(X)}.$$
Using $X^{q}-X=\prod_{r∈\ff{q}}(X-r)$, Berlekamp deduces the
factorization $f(X)=\prod_{r ∈ \ff{q}} \gcd(f(X),h(X)-r)$ \Notes{not
  clear to me}. The method requires to compute $q$ GCD's, hence it
cannot be used for arbitrary finite fields. The root-finding method
called \emph{Berlekamp Trace Algorithm} (BTA)~\cite{berl70} takes
advantage of the factorization
$$X^{q^n}-X=α^{-1} · \prod_{r ∈ \ff{q}}(\Tr_{\ff{q^n}/\ff{q}}(α · X)-r).$$
The roots of $f$ are separated by computing GCD's with the different
factors for varying $α$. This method is recursive, and it may be
applied to large finite fields with small characteristic. It is still
one of the most efficient methods today.

Moenck~\cite{Moenck77} used resultants to determine the $α$'s and
$r$'s leading to non-trivial GCD's in the above methods. He also
introduced the \emph{Subgroup Refinement Algorithm} to pursue further
the factorization of the field equation considered by Legendre by
considering successive subgroups of $\ff{q}^\ast$ for some
special primes $q$.  Rabin~\cite{Rabin79} proposed a method close to
Legendre's. \Notes{talk about Mignotte-Schnorr}

Cantor and Zassenhaus~\cite{cantor1981} suggested to consider random
polynomials $h(X)$ instead of $X+r$ in Legendre's decomposition in
order to factor a polynomial $f$ with factors of equal degree by means
of GCD's. The method is probabilistic and is one the most efficient
methods today.

Berlekamp~\cite{mBER84a} proposed the \emph{Least Affine Multiple
  method} (LAM) which consists in computing the least affine multiple
of $f$, then exhaustively searching the roots of $f$ among the roots
of this polynomial.

Menezes, van Oorschot and Vanstone~\cite{MenezesOV88,OorschotV89}
combined BTA with the LAM method. Moreover, they~\cite{Menvanovans92}
generalized Moenck's Subgroup Refinement Algorithm~\cite{Moenck77} and
introduced the \emph{Affine Refinement Method} (ARM) by mirroring the
successive refinement of a subgroup of $\ff{q}^\ast$ in Moenck's
algorithm by refinement of linear subspaces of $\AG$, the
$n$-dimensional affine geometry of $\ff{q}$.  As described in their
papers, the ARM makes use of the LAM method.

Shoup~\cite{Shoup91b} proposed a deterministic generalization of BTA
for the factorization and root-finding problems. Its worst-case
running time complexity is similar to the average running time of the
best known probabilistic methods such as BTA and Cantor-Zassenhaus.

Niederreiter~\cite{nied94} proposed an algorithm based on the
resolution of differential equations over finite fields.  As mentioned
by several authors~\cite{Fleis96} this algorithm is highly related to
Berlekamp's factoring algorithm.

Recently, Petit~\cite{cgUCL-P14} introduced a method related to BTA
called the \emph{Successive Resultants Algorithm} (SRA), suitable for
fields $\ff{q^n}$ with small $q$. Later, Grenet, van der Hoeven and
Lecerf~\cite{grenet-hoeven-lecerf-roots,grenet2015deterministic}
suggested methods based on Graeffe transforms, adapted for \emph{FFT
  fields}, i.e., fields $\ff{q}$ where $q-1$ has a large $2$-adic
valuation. We call these methods \emph{Graeffe Transform Algorithms}
(GTA). SRA and GTA are similar in that they use resultants to reduce
the root finding problem to one in a smaller space; they are however based
on different mathematical structures, as we shall see in the next
section.


\paragraph{Our contribution}
We give a unified presentation of several of the algorithms above. We
categorize them in two main families: \emph{additive algorithms} based
on the structure of the affine geometry $\AG$, and
\emph{multiplicative algorithms} based on the structure of the
multiplicative group $\ff{q}^\ast$.  We also highlight how SRM and GTA
are the multiplicative counterparts of BTA, ARM and SRA.  Finally, we
analyze the randomized versions of those algorithms when relevant.
Although most relationship between these algorithms have been folklore
for very long, to the best of our knowledge this is the first
systematic treatment of the kind. As a bonus, our classification
enabled us to discover a new algorithm derived from SRA.

Next, we present new variants of these algorithms exhibiting
interesting properties. In particular we provide \emph{hybrid}
algorithms which mix both the additive and the multiplicative
paradigms, retaining the benefits of both. \Notes{We also present
  variants of SOMETHING which improve SOMETHING}

Finally, we discuss non-trivial implementation details, and provide an
implementation of each of the algorithms analyzed in the paper. As far
as we know, this is the only comprehensive implementation covering so
many different root finding algorithms. We use our implementation to
compare the performances of the algorithms in various settings.


\subsection{Notation and complexity model}
\label{sec:compl-model-fund}

In the rest of the paper, $q$ is a prime or prime power, $n$ is an
integer $≥1$, we let $Q=q^n$, and we let $\ff{q},\ff{Q}$ be finite
fields with $q,Q$ elements respectively.  We also let $f$ be a
separable polynomial in $\ff{Q}[X]$ of degree $d$ which splits
completely in $\ff{Q}$. We measure all complexities in the algebraic
complexity model, where the cost of an algorithm is counted in terms
of the number of operations $(+,\times,\div)$ in $\ff{q}$. We use the
Landau notation $O(\ )$ to express asymptotic complexities, and the
notation $\bigOt(\ )$ to neglect logarithmic factors.

We let $\Mul_q(n)$ be the cost of multiplying two polynomials in
$\ff{q}[Z]$ of degree at most $n$, under the assumptions
of~\cite[Chapter~8.3]{Gathen2003}. When $q$ is clear from the context,
we simply write $\Mul(n)$.  Using FFT multiplication, one can take
$\Mul(n)∈ O(n\log(n)\loglog(n))$.  With this notation, elements of
$\ff{Q}$ can be multiplied in $O(\Mul_q(n))$ operations, and their
inverses can be computed in $O(\Mul_q(n)\log n)$ operations.

We also let $M_Q(d)$ be the cost of multiplying two polynomials in
$\ff{Q}[X]$ of degree at most $d$.  Using Kronecker's
substitution~\cite{moenck76,kaltofen87,Gathen2003,GathenS92,harvey09},
this can be done in $O(\Mul_q(dn))$ operations. Euclidean divisions of
degree at most $d$ in $\ff{Q}[X]$ and multiplications in the ring
$\ff{Q}[X]/f(X)$ also cost $O(\Mul_q(dn))$ operations. Finally
computing the GCD of two polynomials of degree at most $d$ costs
$O(\Mul_q(dn)\log(dn))$ operations
(see~\cite[Th.~11.5]{Gathen2003}).

We shall need to evaluate and interpolate polynomials of degree $d$ at
$O(d)$ points of $\ff{Q}$. Using the asymptotically fast algorithms
for evaluation and interpolation in~\cite[Chapter~10]{Gathen2003},
both operations can be performed in $O(\Mul_q(dn)\log d)$
operations.  In Section~\ref{sec:impl-exper-results} we will present
variants of these algorithms for special instances of the problem.

Finally, we let $ω$ be the \emph{exponent of linear algebra}, i.e., a
constant such that $n\times n$ matrices with coefficients in any field
$k$ can be multiplied using $O(n^\omega)$ additions and
multiplications in $n$.


\paragraph{Outline}
The paper is organized as follows. In
Section~\ref{sec:root-find-algor} we give a systematic presentation of
the different root finding algorithms. Then, in
Section~\ref{sec:new-variants} we present variants of the previous
algorithms with interesting properties. Finally, in
Section~\ref{sec:impl-exper-results} present our implementation and
experimental results.




\section{Root-finding algorithms and their relationships}
\label{sec:root-find-algor}

We present here \Notes{keep this updated: six} root finding
algorithms. We make a first classification by dividing them in two
families: \emph{additive} and \emph{multiplicative} algorithms.

Additive algorithms are based on the structure of the \emph{affine
  geometry} $\AG$. They are most efficient when the base field
$\ff{q}$ is small.  Berlekamp's BTA~\cite{berl70}, Menezes, van
Oorschot and Vanstone's ARM~\cite{Menvanovans92}, and Petit's
SRA~\cite{cgUCL-P14} all belong to this family.  We present them in
Section~\ref{sec:affine-geom-algor}.

Multiplicative algorithms are based on the structure of the multiplicative
group $\ff{q}^\ast$ (or $\ff{Q}^\ast$, \emph{mutatis mutandis}). They
are most efficient when the largest prime divisor of $q-1$ is
small. Legendre's method, Moenk's SRM~\cite{Moenck77}, and Grenet, van
der Hoeven and Lecerf's~\cite{grenet2015deterministic} GTA all belong
to this family. We present them in Section~\ref{sec:mult-algor}.

In an orthogonal way, we categorize the previous algorithms by the
algorithmic principle they are based upon. By this criterion, BTA and
Legendre's method are based on intersecting the root set of $f$ with
sets of codimension 1 or index 2, respectively. ARM and SRM are based
on intersecting the root set of $f$ with increasingly finer partitions
of the root space. All four algorithms use GCD computations to perform
the set intersections. Finally, SRA and GTA are based on projecting
the roots of $f$ onto increasingly smaller root spaces, then inverting
the process to get back to the roots of $f$. The projections in both
algorithms are done by means of resultant computations. We will also
highlight how SRA and GTA are in a sense \emph{dual} algorithms to ARM
and SRM respectively. Our classification of root finding algorithms is
summarized in Table~\ref{tab:algorithms}.

\begin{table}
  \centering
  \begin{tabular}{l | c c}
    & Additive & Multiplicative\\
    \hline
    Intersection & BTA~\cite{berl70} & Legendre\\
    Partition refinement & ARM~\cite{Menvanovans92} & SRM~\cite{Moenck77}\\
    Projection & SRA\cite{cgUCL-P14} & GTA~\cite{grenet2015deterministic}
  \end{tabular}
  \caption{Categorization of root finding algorithms by mathematical structure (columns) and algorithmic principle (rows).}
  \label{tab:algorithms}
\end{table}

Finally, randomization plays an important role in all root finding
algorithms, usually by improving average case complexity bounds.  We end
the section by discussing the randomization of each of the previous
algorithms, and by giving details on those algorithms whose
randomization is non-trivial, such as the randomized GTA
of~\cite{grenet-hoeven-lecerf-roots}, and a new randomized variant of
SRA.


\subsection{Additive algorithms}
\label{sec:affine-geom-algor}
\Notes{Some bits from beginning of Section 3 in Christophe's document, to be split between here and  intro of Section 2}

\medskip 

We now present three root finding algorithms based on the structure of $\AG$. Our presentation will not necessarily follow the original ones and algorithmic changes will be explicitly mentioned. 
This first method is called Berlekamp's trace algorithm (BTA)~\cite{berl70} and was designed in the seventies. The second one was developped by Menezes, van Oorschot and Vanstone~\cite{Menvanovans92}
 twenty years later and is called the Affine Refinement Method (ARM). The third method was recently 
 proposed by Petit~\cite{cgUCL-P14} and is called the Successive Resultants Algorithm (SRA). In order to present these methods we need to introduce some basic concepts of affine geometry.

\paragraph{Affine geometry of $\AG$.} The \emph{finite affine geometry} $\AG$ is the set of all vector subspaces of $\basef^\nn$ and their translates. Fixing a basis $\{\upsilon_1,\ldots,\upsilon_\nn\}$ of $\extf$ over $\basef$, we can
identify the elements of $\extf$ with the points of $\AG$. \Notes{not really, just by computing argument} To this basis, we associate the flag of linear
subspaces $V_0\subset V_1\subset \cdots \subset V_\nn$ defined by
\begin{equation}
  V_i = \Span(\upsilon_1,\dots,\upsilon_i),
\end{equation}
so that $\dim V_i = i$ and $\card V_i = \qq^i$.

Let now $\rho\in\extf$, and $r_i\in\fq$ such that $\rho=\sum_j r_j\upsilon_j$.  For any
$V_i$ we define the affine space (also called an $i$-flat)
\begin{equation}
  V_{i,\rho} = V_i + \rho.
\end{equation}
By construction, the reunion of all $i$-flats for any $i$ is
isomorphic to $\AG$, and we call it the \emph{affine geometry of
  $\extf$}.
Observe that $V_{i,\rho}=V_{i,\rho'}$, if and only if $\rho-\rho'\in V_i$. Hence
any $V_{i,\rho}$ can be represented canonically by taking $\rho$ of
the form $\rho=\sum_{j>i}r_j\upsilon_j$. There are exactly
$\qq^{n-i}$ distinct $i$-flats, each of size $\qq^i$. By definition we have
\begin{equation}
  \label{decomposition-tree-iflats}
  V_{i,\rho} = \bigcup_{r_i \in\basef} V_{i-1,\rho + r_i \upsilon_i},
\end{equation}
where $\rho$ may be canonically chosen as $\sum_{j>i}r_j\upsilon_j$.

\medskip

\noindent All the three root finding methods are based on a recursive decomposition of $\extf$ into $i$-flats 
according the above rule. This decomposition leads to a $\qq$-ary tree where the root 
is $V_{n}=\extf$ and each leaf is in one-to-one correspondence with an element of $\extf$.

\medskip


\paragraph{Principle of BTA, ARM, BTARM and SRA methods.} Consider a polynomial $f$ of degree $\dd$ over $\extf$. In BTA and ARM methods, the roots of $f$ are determined by recursively separating the roots of $f$ contained in $V_{i,\rho}$ into sets of roots contained in the different children of this $i$-flat. These methods consist in intersecting sets which is pratically achieved by computing GCD's between minimal polynomials representing these sets. From this procedure the roots of $f$ may be deduced because 
these sets will ultimately contain one element whose (monic) minimal polynomial is linear with the element as a constant term. In BTA method minimal polynomials representing the children of $V_{i,\rho}$
are expressed as a GCD of hyperplanes while in ARM method they are explicitly computed at each stage. A third new variant BTARM combines both approaches.

\medskip
 
SRA method consists in recursively identifying the children of $V_{i,\rho}$ containing roots of $f$. 
This can be practically achieved by determining among the minimal polynomials of the children of 
$V_{i,\rho}$ the ones having a zero resultant with $f$. %This method consists therefore in projecting the roots of $f$ onto the space $V_{i-1}^\ast$.
From this procedure the roots of $f$ may be deduced because the method ultimately determines the leaves containing a root of $f$, which are sets containing a single element.

In order to implement the above general principles practically we need to introduce some 
mathematical tools.

%\subsubsection{Algebraic objects related to the affine geometry AG(\qq,\nn)}
%\label{sec:affine-geom}

\paragraph{Algebraic objects related to BTA, ARM, BTARM and SRA}

To each of the subspaces $V_i$ we associate its minimal polynomial, that we denote by
$L_i$. The polynomials $L_i$ satisfy a recurrence relation 
\begin{equation}
\label{Li_generation}
  L_0(X) = X, \; \;  L_i(X) = (X^\qq - L_{i-1}(\upsilon_i)^{\qq-1}X)\circ L_{i-1}(X)
\end{equation}
(see~\cite[Ch. 11]{mBER84a}). In particular $L_\nn(X)=X^\qn-X$. Notice that, by definition, each $L_i$ defines a linear map
$\extf\to\extf$, with kernel $V_i$. It will be convenient to encode
this information in an $\nn\times\nn$ matrix. We define
$\gamma_{i,j}:=L_i(\upsilon_j)$, and
\begin{equation}
  \label{eq:Gamma}
  \Gamma :=
  \begin{pmatrix}
    \gamma_{0,1} & \cdots & \gamma_{0,\nn}\\
    \vdots & & \vdots\\
    \gamma_{\nn-1,1} & \cdots & \gamma_{\nn-1,\nn}
  \end{pmatrix}.
\end{equation}
Observe that by definition $\gamma_{i,j}=0$ whenever $j\le i$, hence
$\Gamma$ is an upper triangular matrix, associated to a linear map
$\extf\to\extf^\nn$ sending any $\delta\in\extf$ to the vector
$\bigl(L_0(\delta),\dots,\allowbreak L_{n-1}(\delta)\bigr)$. %
The diagonal elements of $\Gamma$ play a special role in the
algorithms we analyze. %
For the rest of this paper, we also set $\beta_i:=\gamma_{i-1,i}$ and
$\alpha_i:=\beta_i^{\qq-1}$ for any $1\le i \le \nn$. %
We deduce a decomposition
\begin{equation}
\label{decomposition_field_eq_gen}
  X^\qn - X = (X^\qq - \alpha_\nn X) \circ \cdots \circ (X^\qq - \alpha_1 X),
\end{equation}
from which we readily obtain a bound on the cost of computing the
matrix $\Gamma$.

\begin{Lem}
  \label{th:gammas}
  Given a basis $\{\upsilon_1,\dots,\upsilon_n\}$ of $\extf$, the
  elements $\gamma_{i,j}$, $\beta_i=\gamma_{i-1,i}$ and
  $\alpha_i=\beta_i^{q-1}$ defined in Eq.~\eqref{eq:Gamma} can all be
  computed using $O(n^2\Mul(n)\log(q))$ operations in $\basef$.
\end{Lem}

We define $V_i^\ast$ as the image space of $L_i$, i.e., the subspace
generated by the elements of the $i$-th row of $\Gamma$.  It is easily verified that
$\dim V_i^\ast=n-i$ and that $\{\gamma_{i,i+1},\dots,\gamma_{i,\nn}\}$
is a basis of $V_i^\ast$. Let $L_i^\ast$ be the minimal polynomial of
$V_i^\ast$. It is shown in~\cite[Ch. 11]{mBER84a} that
$L_i^\ast$ is the unique linearized polynomial such that
\begin{equation}
\label{dual_polynomial}
(L_i^\ast \circ L_i)(X)=(L_i \circ L_i^\ast)(X)=X^\qn-X\,,
\end{equation}
where $L_i^\ast$ is called the \emph{dual} of $L_i$.

\medskip

Another important linear map is the trace application over $\extf$ relative to $\basef$ defined by $\Tr_{\extf/\basef}(X)=\sum_{i=0}^{n-1} X^{\qq^i}$. Its kernel is a subspace of $\extf$ with dimension $n-1$. For any basis $\{\upsilon_1,\ldots,\upsilon_\nn\}$ of $\extf$ over $\basef$, there exists a \emph{dual basis} $\{\nu_1,\ldots,\nu_\nn\}$ satisfying
$$
\Tr_{\extf/\basef}(\upsilon_i \cdot \nu_j)=
\left\{
\begin{array}{ll}
1 & \mbox{if } i=j, \\
0 &  \mbox{otherwise}. \\
\end{array}
\right.
$$
%\paragraph{The affine geometry of $\extf$} 
According to the definition of the dual  basis $\{\nu_1,\ldots,\nu_\nn\}$, the $V_i$ may be represented as 
the intersection of the kernels of the linear maps $X \mapsto \Tr_{\extf/\basef}(\nu_j X )$ for $j>i$. It turns out that the polynomials 
 $L_i$ may be alternatively described as
 \begin{equation}
 \label{L_i_alt_formula}
 L_i(X)=\gcd\bigl(\Tr_{\extf/\basef}(\nu_j X );j>i\bigr) \,.
 \end{equation}
 \Notes{Should be proved}
%$$V_i=\cap_{j > i} \Ker( \Tr_{\extf/\basef}(\beta_j X )) \,. $$
Finally, we define the polynomial $M_{i,\rho}$ as the minimal
polynomial of $V_{i,\rho}$, hence
\begin{equation}
  M_{i,\rho}(X) = L_i(X - \rho) = L_i(X) - \sum_{j>i}r_j\gamma_{i,j}.
\end{equation}
%$V_{i,\rho}=V_{i,\rho'}$
Similarly to the $i$-flats $V_{i,\rho}$, we observe that $M_{i,\rho}=M_{i,\rho'}$ if and only if $\rho-\rho'\in V_i$. Hence
any $M_{i,\rho}$ can be represented canonically by taking $\rho$ of
the form $\rho=\sum_{j>i}r_j\upsilon_j$. The alternative expression of $L_i$ allows us to represent $M_{i,\rho}$ as
\begin{equation}
\label{alternative_M_i_rho}
\begin{array}{lll}
M_{i,\rho}(X)&=& L_i(X-\rho) \\
&=&\gcd\bigl(\Tr_{\extf/\basef}(\nu_j \cdot  (X-\rho) );j>i\bigr)  \\
&=& \gcd\bigl(\Tr_{\extf/\basef}(\nu_j X) -\Tr_{\extf/\basef}(\nu_j \rho) ;j>i\bigr)  \\
&=&  \gcd\bigl(\Tr_{\extf/\basef}(\nu_j X) -r_j ;j>i\bigr)  \\
\end{array}
\end{equation}
Finally, according to the decomposition rule~(\ref{decomposition-tree-iflats}) concerning the $i$-flats $V_{i,\rho}$, we have
\begin{equation}
\label{node_product}
  M_{i,\rho}(X) = \prod_{r_i\in\basef} M_{i-1,\rho+r_i\upsilon_i}(X).
\end{equation}
This defines a decomposition of $X^{\qn}-X$ into a product tree, where
each node at level $i$ corresponds to a $M_{i,\rho}$, defined by a
$\rho=\sum_{j>i}r_j\upsilon_j$, and has $\qq$ children, defined by
the elements $r_i\upsilon_{i}+\rho$ for $r_i \in\basef$. In particular,
the leaves are the linear polynomials $X-\rho$ for all points
$\rho\in\extf$.
%
Note that the product tree depends on the basis $\{\upsilon_1,\ldots,\upsilon_\nn\}$ and is isomorphic to the $q$-ary tree defined in terms of $i$-flats $V_{i,\rho}$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\paragraph{Algebraic description of BTA/ARM/BTARM} These methods consist in \emph{intersecting} the roots of $f$ with the $i$-flats $V_{i,\rho}$, i.e, with the fibers by $L_i$ of each element $\ell_{i,\rho} \in V_i^\ast$.
%The method consists in \emph{projecting} the roots of $f$ onto the spaces $V_i^\ast$ while simultaneously \emph{intersecting} those with the corresponding fibers of $L_i$ identified by $\ell_i$.
This is achieved by computing the non-constant GCDs among the set
% the computation of the minimal polynomial of those intersections by means of gcd's, i.e.
\begin{equation}
\label{base_ARM}
\gcd(L_i(X)-\ell_{i,\rho},f(X)) \mbox{ for any } \ell_{i,\rho} \in V_i^\ast\,,
\end{equation}
for $i$ starting from $n$ and going down to 0.
It ultimately leads to the roots of $f$ because the GCD's satisfying
$$\gcd(L_0(X)-\ell_{0,\rho},f(X))=\gcd(X-\ell_{0,\rho},f(X)) \ne 1$$
yield the linear factors of $f$.
Note that
\begin{equation}
\label{eq:duality-gcd}
\gcd(L_i(X)-\ell_{i,\rho},f(X)) \ne 1
\end{equation}
implies that there is a root of $f$ in $V_{i,\rho}$.
In practice, these GCDs are computed in a recursive way. Let us first define
\begin{equation}
  \label{eq:f_i,rho_def}
  f_{i,\rho}(X)=\gcd(L_i(X)-L_i(\rho),f(X))\,  
\end{equation}
where $\rho \in \extf$. Hence $f_{i,\rho}(X)=f_{i,\rho'}(X)$ if and only if $\rho-\rho' \in V_i$.
Using Eq.~\eqref{base_ARM} and the fact that $V_n^\ast=\{0\}$, $f_{n,0}(X)$ is first computed, i.e.,
$$f_{n,0}(X)=\gcd(L_n(X),f(X))=\gcd(X^{q^\nn}-X,f(X))\,.$$
The recursive step is based on Eq.~\eqref{node_product} and works as follows: for any $L_i(\rho) \in V_i^\ast$ such that $f_{i,\rho}(X)$ is neither a constant polynomial nor a linear factor,
\begin{equation}
\label{eq:f_i,rho_rec}
\begin{aligned}
f_{i,\rho}(X)&=\prod_{c \in \basef} \gcd(M_{i-1,\rho+c \cdot \upsilon_i}(X),f_{i,\rho}(X)) \\
&=\prod_{c \in \basef} \gcd(L_{i-1}(X)-L_{i-1}(\rho+c \cdot \upsilon_i),f_{i,\rho}(X)) \\
&=\prod_{c \in \basef} f_{i-1,\rho+c \cdot \upsilon_i}(X)\\
\end{aligned}
\end{equation}
This procedure builds a tree where at each level only the nodes containing a polynomial of degree strictly greater than 1 have children. Said otherwise, the leafs of the tree are either constant polynomials or the distinct linear factors of $f$ (without their algebraic multiplicity).

\Notes{Include picture?}

\medskip

\noindent It is possible to deduce three variants of this method, i.e. BTA, ARM and a new combined method BTRAM. These algorithms mainly differ by the way the minimal polynomials of the $i$-flats $V_{i,\rho}$ are represented and therefore the way the $f_{i,\rho}(X)$'s are computed at the different stages.

\medskip 
 
\noindent  
 
\subsubsection{Affine Refinement Method (ARM)}
\label{sec:ARM}
 \Notes{Start mainly from Section 3.2 in ISSAC, maybe a bit from Section 3.2 in Christophe}

 
In ARM, the spaces $V_{i}$ are effectively represented by their minimal polynomials $L_i$, and the factors $f_{i,\rho}(X)$'s are recursively computed via Eq.~\eqref{eq:f_i,rho_rec} as
 \begin{equation}
 \label{gcd_ARM}
 f_{i-1,\rho+x\cdot\upsilon_i} = \gcd(L_{i-1}(X)-L_{i-1}(\rho + c \cdot \upsilon_i),f_{i,\rho}(X)) \,.
 \end{equation}
ARM is summarized in Algorithm~\ref{alg:arm}.
%Note that the $l_i$ for which $\gcd(f(X),L_i(X)-l_i) \ne 1$ correspond to the $L_i$-projections onto $V_i^\ast$ of the roots of $f$.

\begin{algorithm}[!h]
\caption{Affine Refinement Method}
\label{alg:arm}
\begin{algorithmic}[1]
\REQUIRE {A polynomial $f\in\extf[X]$ of degree $\dd$,\\
A basis $\{\upsilon_1,\dots,\upsilon_\nn\}$ of $\extf$.}
\ENSURE {The roots of $f$ in $\extf$.}
\STATE\label{alg:arm:beta} Compute $\beta_i = (L_{i-1}(\upsilon_i))_{i<n}$, as described in Lemma~\ref{th:gammas};
\STATE Set $L_0 = X$;
\FOR {$1 \le i < \nn$}
\STATE\label{alg:arm:Li} Compute $L_i = L_{i-1}^\qq - \beta_i^{\qq-1} L_{i-1} \mod f$;
\ENDFOR
\STATE Set $S_n = \emptyset$, $R_n=\{0\}$, and $f_{\nn,0} = f$;
\FOR {$\nn \ge i \ge 1$}\label{alg:arm:for}
\FOR {each $\rho\in R_i$}
\STATE\label{alg:arm:combili} Compute $\ell_{i-1,\rho} = L_{i-1}(\rho)$;
\STATE\label{alg:arm:mod} Compute $\bar{L}_{i-1,\rho} = L_{i-1}\mod f_{i,\rho}$;
\FOR {any $c\in\basef$}
\STATE\label{alg:arm:gcd} Compute $f_{i-1,\rho+c\cdot \upsilon_i} = \gcd(\bar{L}_{i-1,\rho} - \ell_{i-1,\rho} - c\beta_i, f_{i,\rho})$;
\ENDFOR
\ENDFOR
\STATE\label{alg:arm:addroot} Let $S_{i-1} = S_i\cup\{\delta \mid f_{i-1,\rho+c\cdot\upsilon_i}(X) = (X - \delta) \}$;
\STATE\label{alg:arm:nextloop} Let $R_{i-1} = \{\rho \mid \deg f_{i-1,\rho+c\cdot\upsilon_i} > 1\}$
\ENDFOR
\RETURN $S_0$.
\end{algorithmic}
\end{algorithm}

\begin{Lem}
  \label{th:arm:lemma}
  Let $f\in\extf[X]$ be a separable polynomial that splits completely
  in $\extf$. Let $\{\upsilon_1,\dots,\upsilon_n\}$ be a basis of
  $\extf$, and let $L_i$ be the polynomials defined in
  Eq.~\eqref{Li_generation}.

  On input $f$ and $\{\upsilon_1,\dots,\upsilon_n\}$, the sets
  $R_0,\dots,R_n$ and $S_0,\dots,S_n$ computed by ARM satisfy
  $R_i\cap S_i = \emptyset$ and
  $L_i(R_i)\cup L_i(S_i) = \{L_i(\delta) \mid f(\delta) = 0\}$.
\end{Lem}
\begin{proof}
  \Notes{This proof is messy: probably best to use a picture and hand
    waving.}
  
  The polynomials $L_i\mod f$ computed in Step~\ref{alg:arm:Li}
  correspond to their definition in Eq.~\eqref{Li_generation}, and the
  polynomials $f_{i,\rho}$ computed in Step~\ref{alg:arm:gcd}
  correspond to their definition in Eq.~\eqref{eq:f_i,rho_def}.

  By definition, 
  \begin{equation}
    \label{eq:arm:sets}
    \{\delta \mid f(\delta) = 0\} = \bigcup_\rho \{\delta \mid
    f_{i,\rho}(\delta) = 0\}.
  \end{equation}
  By applying $L_i$ to both sides of the equation, we can rewrite
  $\{L_i(\delta) \mid f(\delta) = 0\}$ as
  \begin{equation}
    \label{eq:arm:sets-proj}
    \left(\bigcup_{\deg f_{i,\rho}>1}\{L_i(\delta)\mid f_{i,\rho}(\delta) = 0\}\right) \cup
    \left(\bigcup_{\deg f_{i,\rho}=1}\{L_i(\delta)\mid
      f_{i,\rho}(\delta) = 0\}\right).
  \end{equation}
  But $L_i(\delta)=L_i(\rho)$ for any root $\delta$ of $f_{i,\rho}$,
  we conclude that the first term in Eq.~\eqref{eq:arm:sets-proj} is
  equal to $L_i(R_i)$, and the second term is equal to $L_i(S_i)$.
\end{proof}

\begin{Theo}
  \label{th:arm}
  Algorithm~\ref{alg:arm} is correct. It can be implemented so as to
  run in $O\bigl(n\Mul(dn)(\log(q) + q\log(dn)) + dn^3\bigr)$
  operations in $\basef$ in the worst case, and
  $O\bigl(\Mul(dn)(n\log(q) + q\log(dn)\log(d))\bigr)$ operations on
  average, plus an additional $O(n^2\Mul(n)\log(q))$ operations
  independent of the input $f$.
\end{Theo}
\begin{proof}
  Correctness follows immediately from
  Lemma~\ref{th:arm:lemma}. Indeed by definition
  $\deg f_{0,\rho}\le 1$, thus $R_0$ is empty and $S_0$ contains all
  the roots of $f$.

  The computation of the $\beta_i$'s in Step~\ref{alg:arm:beta} only
  depends on the basis $\{\upsilon_1,\dots,\upsilon_n\}$, and can be
  performed in $O(n^2\Mul_q(n)\log(q))$ operations in $\basef$, as
  described in Lemma~\ref{th:gammas}. We now analyze the core of the
  algorithm.

  \paragraph{Worst-case complexity.} We set $Q=q^n$.  The degree of the
  polynomials in Step~\ref{alg:arm:Li} is always bounded by the degree
  $d$ of $f$, hence each $L_i$ is computed using
  $O(\Mul_Q(d)\log(q))$ operations in $\basef$, yielding a total of
  $O(n\Mul_q(dn)\log(q))$ for all of them.

  We now analyze the cost of each iteration of Loop~\ref{alg:arm:for}
  separately. For a fixed $i$, we give the costs of
  Steps~\ref{alg:arm:combili},~\ref{alg:arm:mod}
  and~\ref{alg:arm:gcd}.

  There are many ways to compute the values $\ell_{i,\rho}$ in
  Step~\ref{alg:arm:combili}. We choose to represent the
  approximations $\rho\in R_i$ by their coefficient vector over the
  basis $\upsilon_1,\dots,\upsilon_n$, so that
  \[
    \rho = \sum_{j=1}^n r_j\upsilon_j 
    \quad\text{and}\quad
    \ell_{i-1,\rho} = \sum_{j=1}^n r_j\gamma_{i-1,j}.
  \]
  The linear combination on the right requires at most $n^2$ products
  in $\basef$, and there are at most $d$ such values to compute, hence
  the total cost for this step is $O(dn^2)$ operations in $\basef$.

  The modular reductions in Step~\ref{alg:arm:mod} can be grouped
  together for each $i$. %
  Because all degrees are bounded by $d$, putting the polynomials
  $f_{i,\rho}$ in a binary subproduct tree structure as
  in~\cite[Lemma~10.4]{Gathen2003} allows one to compute all
  $L_{i-1,\rho}$ using $O(\Mul_q(dn)\log(d))$ operations.

  In Step~\ref{alg:arm:gcd} the degrees of the polynomials
  $f_{i,\rho}$ sum up to $d$. %
  For each of these, we compute $q$ GCDs with polynomials of similar
  degree. %
  Each GCD costs $O(\Mul_q(d_\rho n)\log(d_\rho n))$ operations, where
  $d_\rho$ is the degree of $f_{i,\rho}$. %
  Summing over all $\rho$, and using the super-linearity of the
  function $\Mul$, we bound this cost by $O(q\Mul_q(dn)\log(dn))$
  operations.

  In the worse case, Loop~\ref{alg:arm:for} is repeated $n$ times,
  thus the overall cost for the whole algorithm is bounded by
  $O(n\Mul_q(dn)\log(q) + nq\Mul_q(dn)\log(dn) + dn^3)$ operations in
  $\basef$.

  \paragraph{Average-case complexity.} When all $f_{i,\rho}$ have
  degree $\le 1$, the sets $S_i$ contain all roots of $f$, and the sets
  $R_i$ become empty; thus the algorithm quickly terminates without
  any further computation. %
  It follows that the average complexity is determined by the average
  depth at which the flats $V_{i,\rho}$ separate the roots of $f$. %
  As already observed by Menezes, Van Oorschot and
  Vanstone~\cite[Theorem~7]{Menvanovans92}, this depth is related to
  the height of a data structure called \emph{digital trie}, and is
  bounded on average by $2\log_q(d) + O(1)$.

  Hence we can assume that on average Loop~\ref{alg:arm:for}
  terminates after $O(\log(d))$ iterations. %
  But then, we can refine the analysis of Step~\ref{alg:arm:combili}
  even further. %
  Indeed, let $h$ be the depth at which ARM stops, note that at each
  iteration the last $n-h$ coefficients in the vector of $\rho$ are
  all zero. %
  Hence, the cost of evaluating $\ell_{i-1,\rho}$ for all $\rho$ is of
  $O(dn\log(d))$ operations, but this is negligible in front of the
  factor $O(q\Mul_q(dn)\log(dn))$ coming from Step~\ref{alg:arm:gcd}.

  In conclusion, the overall complexity of ARM is bounded on average
  by $O\bigl(n\Mul_q(dn)\log(q) + q\Mul_q(dn)\log(dn)\log(d)\bigr)$
  operations in $\basef$.
\end{proof}

\noindent {\bf Remark}: originally~\cite{van1989geometric} expression~(\ref{gcd_ARM}) was computed in three steps. First the Least Affine Multiple of 
$f_{i,\rho}(X)$ was computed, i.e. $LAM(f_{i,\rho}(X))$ which is  the smallest degree monic affine polynomial $F$ such that $f_{i,\rho}(X)|F$. Then the gcd between to affine polynomials was determined, i.e. 
$$G_c(X)=\gcd(L_{i-1}(X)-L_{i-1}(\rho+c \cdot \upsilon_i),LAM(f_{i,\rho}(X)) ) $$
Finally, expression~(\ref{gcd_ARM}) was derived by computing
$$\gcd(G_c(X), f_{i,\rho}(X))\,.$$
%\Notes{the reason of this modification should be detailled probably. May be define LAM with the other notions (Trace, dual basis ect). Note that there is 
%another two variants, one with shifted $f_{i,\rho}(X)$ and another without this shift}
When fast arithmetic is not available, \cite{van1989geometric} shows that computing the gcd in this way is computationally more efficient when $d<n$. Our modification of their algorithm is justified as fast arithmetic is common today.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Berlekamp's Trace Algorithm (BTA)}
\label{sec:BTA}
In BTA, the spaces $V_{i}$ are not effectively represented by their minimal polynomials $L_i$. They are instead expressed as an intersection of hyperplanes, thus giving an alternative way to compute the $f_{i,\rho}(X)$'s. Indeed, according to expression~\eqref{alternative_M_i_rho}, if $\rho=\sum_{j>i}r_j\upsilon_j$ then
$$L_i(X)-L_i(\rho)=\gcd\bigl(\Tr_{\extf/\basef}(\nu_j X) -r_j ;j>i\bigr)\,.$$
Also, $L_{i-1}(X)-L_{i-1}(\rho +c \cdot \upsilon_i)$ may be expressed as
$$
\gcd\bigl(\gcd\bigl(\Tr_{\extf/\basef}(\nu_j X) -r_j ;j>i\bigr), \Tr_{\extf/\basef}(\nu_i X) -c  \bigr).
$$
Therefore, 
\begin{equation}
\label{alt_formula}
L_{i-1}(X)-L_{i-1}(\rho +c \cdot \upsilon_i)=
\gcd\bigl(L_i(X)-L_i(\rho), \Tr_{\extf/\basef}(\nu_i X) -c  \bigr).
\end{equation}
By definition 
$$f_{i,\rho(X)}=\gcd(L_i(X)-L_i(\rho),f(X))\,.$$ 
Thus, $f_{i,\rho(X)}=\gcd(L_i(X)-L_i(\rho),f_{i,\rho}(X))$.
We conclude that 
\begin{equation}
\label{eq:bta:key}
\begin{aligned}
f_{i-1,\rho+c \cdot \upsilon_i}(X)&=\gcd(L_{i-1}(x)-L_{i-1}(\rho +c \cdot \upsilon_i) ,f_{i,\rho}(X)  ) \\
&=  \gcd(  \gcd\bigl(L_i(X)-L_i(\rho), \Tr_{\extf/\basef}(\nu_i X) -c  \bigr)     ,  f_{i,\rho}(X) )    \\
&=  \gcd( \Tr_{\extf/\basef}(\nu_i X) -c ,  f_{i,\rho}(X) )    \\
\end{aligned}
\end{equation}
BTA is summarized in Algorithm~\ref{alg:bta}.

\begin{algorithm}
\caption{Berlekamp's Trace Algorithm (BTA)}
\label{alg:bta}
\begin{algorithmic}[1]
\REQUIRE {A polynomial $f\in\extf[X]$ of degree $\dd$,\\
A basis $\{\nu_1,\dots,\nu_\nn\}$ of $\extf$.}
\ENSURE {The roots of $f$ in $\extf$.}
\STATE Set $S_n=\emptyset$, $R_n={0}$, and $f_{\nn,0} = f$;
\FOR {$\nn \ge i \ge 1$}\label{alg:bta:for}
\STATE\label{alg:bta:prod} Compute $H_i(X) = \Tr_{\extf/\basef}(\nu_i X) \mod \prod_{\rho\in R_i}f_{i,\rho}(X)$;
\FOR {each $\rho\in R_i$}
\FOR {any $c\in\basef$}
\STATE\label{alg:bta:gcd} Compute $f_{i-1,\rho+c\cdot \upsilon_i} =\gcd( H_i(X)-c ,f_{i,\rho}(X) )$;
\ENDFOR
\ENDFOR
\STATE Let $S_{i-1} = S_i\cup\{\delta \mid f_{i-1,\rho+c\cdot\upsilon_i}(X) = (X-\delta)\}$;
\STATE Let $R_{i-1} = \{\rho \mid \deg f_{i-1,\rho+c\cdot\upsilon_i} > 1\}$.
\ENDFOR
\RETURN $S_0$.
\end{algorithmic}
\end{algorithm}

\begin{Lem}
  \label{th:bta_eq_arm}
  Let $f\in\extf[X]$ be a separable polynomial that splits completely
  in $\extf$. Let $\{\upsilon_1,\dots,\upsilon_n\}$ be a basis of
  $\extf$, and let $\{\nu_1,\dots,\nu_n\}$ be its dual basis.

  On input $f$ and $\{\upsilon_1,\dots,\upsilon_n\}$ ARM computes the
  same sets $R_0,\dots,R_n$ and $S_0,\dots,S_n$ as does BTA on input
  $f$ and $\{\nu_1,\dots,\nu_n\}$.
\end{Lem}
\begin{proof}
  This is an immediate consequence of Eqs.~\eqref{gcd_ARM}
  and~\eqref{eq:bta:key}.
\end{proof}

\begin{Theo}
  Algorithm~\ref{alg:bta} is correct. It can be implemented so as to run in
  $O\bigl(n\Mul(dn)(n\log(q) + q\log(dn))\bigr)$ operations in
  $\basef$ in the worst case, and
  $O\bigl(\Mul(dn)(n\log(q) + q\log(dn))\log(d)\bigr)$ operations on
  average.
\end{Theo}
\begin{proof}
  Correctness follows immediately from Lemma~\ref{th:bta_eq_arm} and
  Theorem~\ref{th:arm}, since the root set $S_0$ coincides with the one
  returned by ARM.

  We analyze the cost of each iteration of
  Loop~\ref{alg:bta:for}. The polynomial
  $\prod_{\rho\in R_i}f_{i,\rho}$ has degree at most $d$, and can be
  computed by arranging all polynomials $f_{i,\rho}$ in a binary
  subproduct tree structure as in~\cite[Lemma~10.4]{Gathen2003}, at a
  cost of $O(\Mul_q(dn)\log(d))$ operations in $\basef$.
  
  Then, the polynomial $H_i(x)$ in Step~\ref{alg:bta:prod} can be
  computed by $O(n)$ exponentiations of polynomials of degree at most
  $d$ to the power $q$, for a total cost of $O(n\Mul_q(dn)\log(q))$.

  Using the same subproduct tree structure computed above, all the
  polynomials $H_i(X)\mod f_{i,\rho}(X)$ in Step~\ref{alg:bta:gcd} can
  be computed using again $O(\Mul_q(dn)\log(d))$ operations. 

  Finally, for each $\rho$ we compute $q$ GCDs of polynomials of
  degree $d_\rho$, at a cost of $O(q\Mul_q(d_\rho n)\log(d_\rho n))$
  operations. Using the superlinearity of the function $\Mul$, and the
  fact that $\sum d_\rho \le d$, we conclude that
  Step~\ref{alg:bta:gcd} contributes $O(q\Mul_q(dn)\log(dn))$
  operations to the total complexity.
  
  Adding all terms together, we see that one iteration of
  Loop~\ref{alg:bta:for} costs at most
  $O\bigl(\Mul_q(dn)(n\log(q) + q\log(dn))\bigr)$ operations in
  $\basef$. The worst-case complexity estimate is then evident, and
  the average-case follows easily from Lemma~\ref{th:bta_eq_arm}.
\end{proof}



\subsubsection{BTRA}
%\Notes{See whether we have something competitive and worth mentioning; mention link with Moenck}


There are some alternative methods that I should develop. Again, it is an alternative way to compute equation~(\ref{gcd_ARM}), i.e.
$$\gcd(L_{i-1}(X)-L_{i-1}(\rho + c \cdot \upsilon_i),f_{i,\rho}(X)) \,,$$
We may evaluate $L_{i-1}(X)-L_{i-1}(\rho + c \cdot \upsilon_i)$ first (like in ARM) and then the gcd. There two way's to compute this expression.
\begin{enumerate}
\item Using formula~(\ref{alt_formula})
$$
L_{i-1}(X)-L_{i-1}(\rho +c \cdot \upsilon_i)=
\gcd\bigl(L_i(X)-L_i(\rho), \Tr_{\extf/\basef}(\nu_i X) -c  \bigr) \,.$$
The main advantage is that you don't need to compute the $\gamma_{i,j}$ anymore.
\item Using formula~(\ref{L_i_alt_formula})
$$ L_{i-1}(X)=\gcd\bigl(L_i(X),\Tr_{\extf/\basef}(\nu_i X )\bigr) \,.$$
 then follow the approach of ARM.
\end{enumerate}
of course we need to analyze precsiely the complexity of the algorithm computing the gcd of two affine porlynomials.


\subsubsection{The Successive Resultants Algorithm}
\label{sec:SRA}
\Notes{Write intro to SRA. Define the matrix $A$ properly.}

\begin{algorithm}
\caption{Successive Resultants Algorithm (SRA)}
\label{alg:sra}
\begin{algorithmic}[1]
\REQUIRE {A polynomial $f\in\extf[X]$ of degree $\dd$,\\
A basis $\{\upsilon_1,\dots,\upsilon_\nn\}$ of $\extf$.}
\ENSURE {The roots of $f$ in $\extf$.}
\STATE\label{alg:sra:beta} Compute $\beta_i = (L_{i-1}(\upsilon_i))_{i<n}$ and $\alpha_i=\beta_i^{q-1}$, as described in Lemma~\ref{th:gammas};
\STATE\label{alg:sra:h90} Compute the pseudo-inverse matrix $A$ of the linear map $X^q-X$ on the basis $\{\upsilon_1,\dots,\upsilon_n\}$.
\STATE Let $f^{(0)} = f$;
\FOR {$1 \le i \le n$}
\STATE\label{alg:sra:res} Compute $f^{(i)}(Y) = \Res_X(X^q-\alpha_iX-Y, f^{(i-1)}(X))$;
\ENDFOR
\STATE Set $T_{n}=\{0\}$;
\FOR {$n \ge i \ge 1$}\label{alg:sra:loop}
\FOR {each $\ell_{i,\rho}\in T_i$}
\STATE\label{alg:sra:gcd} Compute $f_{\rho}^{(i-1)}(X)=\gcd(f^{(i-1)}(X), X^q - \alpha_iX - \ell_{i,\rho})$;
\STATE\label{alg:sra:roots} Compute $T_{i-1,\rho} = \{\ell_{i-1,\rho+c\cdot\upsilon_i} \mid f_\rho^{(i-1)}(\ell_{i-1,\rho+c\cdot\upsilon_i})=0\}$;
\ENDFOR
\STATE Set $T_{i-1} = \bigcup T_{i-1,\rho}$.
\ENDFOR
\RETURN $T_0$.
\end{algorithmic}
\end{algorithm}

\begin{Lem}
  \label{th:sra_eq_arm}
  Let $f\in\extf[X]$ be a separable polynomial that splits completely
  in $\extf$. Let $\{\upsilon_1,\dots,\upsilon_n\}$ be a basis of
  $\extf$, and let $L_i$ be the polynomials defined in
  Eq.~\eqref{Li_generation}.

  Let $R_0,\dots,R_n$ and $S_0,\dots,S_n$ be the sets computed by ARM
  on input $f$ and $\{\upsilon_1,\dots,\upsilon_n\}$, then the sets
  $T_0,\dots,T_n$ computed by SRA on the same input satisfy
  $T_i = L_i(R_i)\cup L_i(S_i)$.
\end{Lem}
\begin{proof}
  Let $S_0 = \{\delta\mid f(\delta)=0\}$ be the root set of $f$. %
  By definition the roots of $f^{(i)}$ are precisely the elements of
  $L_i(S_0)\subset V_i$, and Lemma~\ref{th:arm:lemma} shows that
  $L_i(S_0) = L_i(R_i)\cup L_i(S_i)$.

  By construction $f^{(n)}(X)=X^d$, and $T_n$ contains all its
  roots. %
  We prove by induction that $T_i$ contains all the roots of
  $f^{(i)}$. %
  Let $\delta\in S_0$, then $\ell_{i,\rho}=L_i(\delta)$ is in $T_i$. %
  By Eq.~\eqref{Li_generation}, $\ell_{i-1,\rho+c\cdot\upsilon_i}=L_{i-1}(\delta)$ is
  a root of the equation $X^q - \alpha_iX - \ell_{i,\rho}$, and by
  construction it is also a root of $f^{(i-1)}$, thus it is a root of
  $f_\rho^{(i-1)}$. %
  Hence, $L_{i-1}(\delta)$ is inserted into $T_{i-1}$.
\end{proof}

\begin{Theo}
  \label{th:sra}
  Algorithm~\ref{alg:sra} is correct. It can be implemented so as to
  run in $O\bigl(n\Mul_q(dnq)\log(dnq)\bigr)$ operations in
  $\basef$, plus an additional $O(n^2\Mul(n)\log(q))$ operations
  independent of the input $f$.
\end{Theo}
\begin{proof}
  Correctness follows immediately from Lemma~\ref{th:sra_eq_arm},
  indeed $T_0$ is equal to the set $S_0$ computed by ARM.
  
  The $\beta_i$'s and $\alpha_i$'s in Step~\ref{alg:sra:beta} can be
  computed using $O(n^2\Mul_q(n)\log(q))$ operations in $\basef$, as
  described in Lemma~\ref{th:gammas}. The matrix of the application
  $X^q-X$ can be computed in $O(n\Mul_q(n)\log(q))$ operations, and
  its pseudo-inverse in $O(n^3)$ operations. Both computations only
  depend on the basis $\{\upsilon_1,\dots,\upsilon_n\}$.

  All the resultants in Step~\ref{alg:sra:res} have degree $d$. %
  By evaluating and interpolating at $d$ points of $\extf$, we can
  reduce them to $d$ computations of univariate resultants of the form
  \[\Res(X^q - \alpha_iX - \zeta, f^{(i-1)}(X)).\]
  To compute all these resultants efficiently, we reduce $f^{(i-1)}$
  modulo all the $X^q-\alpha_iX-\zeta$ simultaneously, using $O(q)$
  binary subproduct trees as in~\cite[Lemma~10.4]{Gathen2003}, for a
  total cost of $O(q\Mul_q(dn)\log(d))$ operations in $\basef$. %
  The individual resultants can then be computed using
  $O(\Mul_q(nq)\log(nq))$ operations each \Notes{give reference}, and
  the final interpolation costs $O(\Mul_q(dn)\log(d))$ operations. %
  Slightly simplifying these terms, we obtain a total of
  $O\bigl(n\Mul_q(dnq)\log(dnq)\bigr)$ operations to
  compute all $f^{(i)}$'s.

  In Step~\ref{alg:sra:gcd} we start by reducing $f^{(i-1)}$ modulo
  all $X^{q}-\alpha_iX-\ell_{i,\rho}$ simultaneously; $T_i$ contains
  at most $d$ elements, thus the $f_\rho^{(i-1)}$'s can be computed
  using again $O(q)$ binary subproduct trees for a cost of
  $O(q\Mul_q(dn)\log(d))$ operations in $\basef$. %
  Then, we compute each GCD individually at a cost of
  $O(\Mul_q(nq)\log(nq))$ operations for all of them. %
  Summing over all iterations of Loop~\ref{alg:sra:loop}, and
  simplifying slightly, we obtain the same cost as for
  Step~\ref{alg:sra:res}.

  Finally, we have various options to determine the root sets
  $T_{i-1,\rho}$ in Step~\ref{alg:sra:roots}. %
  In most cases, the polynomials $f_\rho^{(i-1)}$ have degree 1, thus
  their only root is immediately obtained at no cost. %
  When $\deg f_\rho^{(i-1)}>1$, we enumerate the roots of
  $X^q-\alpha_iX-\ell_{i,\rho}$ using Hilbert's theorem 90, and
  evaluate $f_\rho^{(i-1)}$ on each. %
  To this extent, we apply the change of variables $X=Y\beta_i$ and
  solve the equation $Y^q-Y=\ell_{i,\rho}/\beta_i^q$ using the
  precomputed matrix $A$. %
  The matrix-vector product gives one root at a cost of $O(n^2)$
  operations, and the other roots are computed in $O(qn)$
  operations. %
  Then, if we let $d_{i-1,\rho}=\deg f_\rho^{(i-1)}$, evaluating
  $f_\rho^{(i-1)}$ on the roots costs
  $O(d_{i-1,\rho}q\Mul_q(n)\log(n))$ operations. %
  Observe that at each iteration
  \[\#T_{i-1}-\#T_i = \sum_{d_{i-1,\rho}>1} (d_{i-1,\rho}-1).\]
  But, since $\#T_0-\#T_n=d-1$, we conclude that
  \[\sum_{i=0}^n\,\sum_{d_{i-1,\rho}>1}d_{i-1,\rho}<2d.\]
  Hence, summing the cost of solving Hilbert 90 over all iterations of
  Loop~\ref{alg:sra:loop}, we obtain a cost of
  $O(dn^2+dq\Mul(n)\log(n))$ operations, which is negligible compared
  to the previous two steps.
\end{proof}

Remarks:
\begin{itemize}
\item In the original paper, Hilbert 90 was replaced by BTA, which
  gave a slightly messier complexity estimate.
\item An easy modification of the algorithm only executes
  $n-\lceil\log(d)\rceil$ iterations of the loops, thus replacing a
  factor $n$ with a factor $n-\log(d)$ in the complexity.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%









\subsection{Multiplicative algorithms}
\label{sec:mult-algor}
\Notes{Discuss parallelisms with Section~\ref{sec:affine-geom-algor}}\\
\noindent\Notes{Maybe move first two paragraphs of Section 4.1 in Christophe's document}

\subsubsection{Moenck's algorithm}
\label{sec:Moenck}
\Notes{Section 4.1 in Christophe's document, maybe move first two paragraphs to intro of Section~\ref{sec:mult-algor}}\\
\noindent \Notes{Careful: the extension described was already done by MOV}\\
\noindent\Notes{+ Mignotte-Schnorr ?}

\subsubsection{Successive Resultants approach}
\label{sec:LIX}
\Notes{Rewrite Section 4.2 in Christophe's document to enhance links with LIX deterministic (not real algorithm)}



\subsection{Randomized variants}
\label{sec:randomized-variants}

\Notes{Some intro}

\subsubsection{Legendre}
\label{sec:legendre}

\subsubsection{Randomized BTA}
\label{sec:BTArand}

\subsubsection{Randomized BTRA}
\label{sec:BTRArand}
\Notes{??}

\subsubsection{LIX algorithm}
\label{sec:LIXrand}

\subsubsection{Randomized SRA}
\label{sec:SRArand}




\section{New algorithm variants}
\label{sec:new-variants}

\subsection{Hybrid algorithms}
\label{sec:hybrid-algorithms}

\subsubsection{ARM-Moenck hybrid}
\subsubsection{SRA-LIX hybrid}
\Notes{??}



\subsection{SRA and ARM with improved worst case complexity}
\label{sec:worst-case}


\subsection{ARM variant with low precomputation}
\label{sec:low-precomputation}

While all the methods presented in Section~\ref{sec:affine-geom-algor} are essentially quadratic in $n$ in the average case, ARM and SRA additionally need some cubic pre-processing time to compute the values $\alpha_i$ or $\beta_i=\gamma_{i-1,i}$.
%
In this section we show how this cost can be avoided for ARM in the average case. %Interestingly, the resulting algorithm share similarities with both ARM and BTA.

In what follows, $h$ is an \emph{a priori} upper bound on the expected number of iterations of ARM, typically $c \cdot \log_\qq d$ with $c \ge 2$. 
%
In the average case, ARM only needs the last $h$ values $\alpha_i$ to split $f$ completely.
%
We observe that these last $\alpha_i$ values can be computed without computing all the other ones. Indeed 
%As seen in Eq.~\eqref{dual_polynomial}, 
for any linearized polynomial $L_i$ there exists a unique \emph{dual} linearized polynomial $L_i^\ast$ such that $$(L_i^\ast\circ L_i)(X)=(L_i\circ L_i^\ast)(X)=X^{\qq^\nn}-X.$$
%Any field decomposition leads therefore to several decompositions, i.e. for any $i$
%$$X^{\qq^\nn}-X=(X^{\qq}-\alpha_\nn) \circ \cdots \circ (X^{\qq}-\alpha_1 X)=L_i(X) \circ L_i^\ast(X)$$
%where $L_i^\ast(X)=(X^{\qq} -\alpha_n X) \circ \cdots \circ (X^{\qq}-\alpha_{i-1}X)$.
%
We can therefore compute suitable $\alpha_i$ values for $L_{n-h}^\ast(X)$ using any basis $\{\upsilon_{1},\ldots,\upsilon_h\}$ of a vector space of dimension $h$ of $\extf/\basef$, and deduce that there exists a decomposition
$$X^{\qq^\nn}-X=(X^{\qq}-\alpha_n) \circ \cdots \circ (X^{\qq}-\alpha_{n-h+1} X) \circ L_{n-h}(X)\,.$$
At this point the elements $\alpha_i$ are known, and the polynomial $L_{n-h}(X)$ is unique but not yet computed. %This requires $O(h^2\log\qq)$ operations in $\extf$.

For i starting from $\nn$ and going down to $n-h$, we then recursively compute polynomials $L_i(X)$ which satisfy
$$X^{\qq^\nn}-X= (X^{\qq}-\alpha_n) \circ \cdots \circ (X^{\qq}-\alpha_{i-1} X) \circ L_{i}(X)\,.  $$
or equivalently $L_{i+1}(X)=L_i(X)^q-\alpha_{i+1}L_i(X)$. 
%
As the polynomials $L_i(X)$ are linearized polynomials, each of these equations reduces to
a bidiagonal system. %, which may be solved at the cost $O(\nn \log q)$ or $O(h \nn \log q)$ for the $h$ polynomials $L_i$.

Note that Step~\ref{alg:arm:Li} of Algorithm~\ref{alg:arm}, which consists in computing $L_i \bmod f$, needs to be slightly modified as only the last $L_i$ polynomial have been computed. %In Algorithm~\ref{} we first recursively compute the values $X^{q^j}\bmod f$, and then linearly combine these values.
%The values $X^{q^j}\bmod f$ are first computed in $O(\Mul(\dd)n\log\qq)$ operations, then $L_i\bmod f$ can be computed in $O(\dd \nn)$ operations. This does not change the complexity of the step. 
We first compute the polynomials $(X^{q^j}\bmod f)$ recursively and deduce the polynomials $(L_i\bmod f)$ using linear combinations.
%
Step \ref{alg:arm:combili}  also needs to be modified: we now compute the values $\ell_{i-1, \rho}= L_{i-1}(\rho)$ by simple evaluation of the polynomial $L_{i-1}$ for any any root approximation $\rho=\sum_j r_j \upsilon_j$ candidate appearing in Step~\ref{alg:arm:gcd}. %Each value $\ell_{i-1, \rho}$ requires $\bigO(\nn \log \qq)$ operations and there are at most $d \cdot h$ such values to compute. The overall complexity for the computation of these values is $O(h \nn\dd  \log \qq )$ which is not a bottleneck in the average case. 
 
 Depending on where the roots of $f$ are located, the a priori bound $h$ may in fact not be sufficient to completely factor $f$. In this case, the algorithm should be launched on the remaining factors of $f$ with a newly generated vector basis with $h'$ elements. The value $h'$ should be chosen larger than $\log_{\qq} \max(\deg f_{h,\rho})$ where $f_{h,\rho}$ are the remaining factors of $f$ at level $h$.

This description leads to Algorithm~\ref{alg:armnoprec} and to the following theorem.

\begin{Theo}
  \label{th:armnoprec}
  Algorithm~\ref{alg:armnoprec} is correct. When $h=O(\log(d))$ it can be implemented so as to
  run in 
  $O\bigl(\Mul(dn)(n\log(q) + q\log(dn)\log(d))  + \Mul(n)dn\log(d)\log(q)  \bigr)$ 
  operations in $\basef$ on
  average.
\end{Theo}
\begin{proof}
Step~\ref{alg:armnoprec:alphai} requires $\bigO(h\log(q) \Mul(n))$ operations per $\alpha_i$ or $\bigO(h^2\log(q) \Mul(n))$ in total.
Step~\ref{alg:armnoprec:Xqj} requires $\bigO(\log(q)\Mul(dn))$ operations for each value of $j$ or $\bigO(n\log(q)\Mul(dn))$ operations in total.
In Step~\ref{alg:armnoprec:Li}, the coefficients of $L_i$ can be computed recursively starting from the smaller degree coefficient. We neglect the cost of inverting $\alpha_i$, after which each coefficient requires $\bigO(1)$ multiplications over $\fqn$ for a total of $\bigO(n\Mul(n))$ operations.
Step~\ref{alg:armnoprec:Limod} requires $\bigO(dn\Mul(n))$ operations. Steps~\ref{alg:armnoprec:Li} and~\ref{alg:armnoprec:Limod} will be executed $h$ times for a total of $\bigO(hdn\Mul(n))$ operations.
Therefore the total cost before the main loop is $\bigO(h^2\log(q)\Mul(n)+n\log(q)\Mul(dn)+hdn\Mul(n))$.

 
Step~\ref{alg:arm-low:combili} requires $\bigO(n\log(q)\Mul(n))$ each time and $\bigO(dhn\log(q)\Mul(n))$ in total. The total cost related to the gcds is $O(q\Mul_q(dn)\log(dn))$ as in Theorem~\ref{th:arm}.
\end{proof}



While our motivation for Algorithm~\ref{alg:armnoprec} was to remove the precomputation costs of ARM, it is interesting to notice that it can also be seen as a generalization of BTA, which corresponds to setting an a priori bound $h=1$ and repeating the algorithm on smaller factors until they are all split.




\begin{algorithm}
\caption{ARM variant with low precomputation}
\label{alg:armnoprec}
\begin{algorithmic}[1]
\REQUIRE {A polynomial $f\in\extf[X]$ of degree $\dd$,\\
An a priori bound $h$ on the height of the tree,\\
A basis $\upsilon_1,\dots,\upsilon_h$ for a vector space of dimension $h$.}
\ENSURE {Partial factorization of $f$, and a factorization in linear terms when the a priori bound $h$ is correct.}
\STATE Set $L_0(X)=X$;
\FOR {$1 \le i \le h$}
\STATE\label{alg:armnoprec:alphai} Compute $\alpha_i=L_{i-1}(v_i)^{q-1}$;
\ENDFOR
\STATE Set $(\alpha_n,\ldots,\alpha_{n-h+1})\leftarrow(\alpha_h,\ldots,\alpha_1)$
%
\FOR{$1\leq j\leq n$}
\STATE\label{alg:armnoprec:Xqj} Compute $(X^{q^j}\bmod f)$ from $(X^{q^{j-1}}\bmod f)$
\ENDFOR
%
\STATE Set $L_n(X)=X^{q^n}-X$
\FOR{$n-1\ge i\ge n-h$}
\STATE\label{alg:armnoprec:Li} Compute $L_i$ such that $L_{i+1}(X)=(X^q-\alpha_iX)\circ L_i(X)$
\STATE\label{alg:armnoprec:Limod} Compute $(L_i\bmod f)$ using computed values $(X^{q^j}\bmod f)$
\ENDFOR
\STATE Set $S=\{\}$ and $R_n=\{0\}$, and $f_{\nn,0} = f \mod L_\nn$;
\FOR {$\nn \ge i \ge n-h$}\label{alg:arm:for}
\FOR {each $\rho\in R_i$}
\STATE\label{alg:arm-low:combili} Compute $\ell_{i-1,\rho} = L_{i-1}(\rho)$;
\STATE\label{alg:arm-low:mod} Compute $\bar{L}_{i-1,\rho} = L_{i-1}\mod f_{i,\rho}$;
\FOR {any $c\in\basef$}
\STATE\label{alg:arm-low:gcd} Compute $f_{i-1,\rho+c\cdot \upsilon_i} = \gcd(\bar{L}_{i-1,\rho} - \ell_{i-1,\rho} - c\gamma_{i-1,i}, f_{i,\rho})$;
\STATE\label{alg:arm-low:addroot} \textbf{if} $f_{i-1,\rho+c\cdot \upsilon_i}$ has degree 1, \textbf{then} add it to $S$;
\ENDFOR
\ENDFOR
\STATE\label{alg:arm-low:nextloop} Let $R_{i-1} = \{\rho \mid \deg f_{i-1,\rho} > 1\}$
\ENDFOR
\FOR{$\rho$ in $R_{n-h}$} 
\STATE	Add $f_{n-h,\rho}$ to $S$ 
\ENDFOR
\RETURN $S$. %\Notes{Adapt output, maybe change indices in line 2-3 to remove line 4}
\end{algorithmic}
\end{algorithm}


\subsection{Composite degree extensions}
\label{sec:composite-degree}


\subsection{Coset intersection variant of Moenck}
\label{sec:SRAMoenck}
\Notes{Section 4.3 in Christophe's document. Remove?}



\section{Implementation and experimental results}
\label{sec:impl-exper-results}

\begin{itemize}
\item $\gamma_{ij}$ precomputation
\item faster resultant
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{refs}
\bibliographystyle{alpha}

\end{document}



% Local Variables:
% ispell-local-dictionary:"american"
% End:


%  LocalWords:  affine subspaces linearized factorizations
%  LocalWords:  iteratively
